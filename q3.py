# -*- coding: utf-8 -*-
"""Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SVyn9gnvrg05WO-HLLjD9f95tlWTCCRp
"""

import logging
import os

import numpy as np
import torch
import torch.nn as nn
import yaml
from torchvision.utils import save_image
import torchvision.datasets
from torchvision import transforms as tf

from dataset import NewSimpleDataset
from model import ModelConfig, ConvolutionGenerator, LinearGenerator
from trainer import TrainConfig
from utils import set_seed
from classifier.model import CNNClassifier
from torch.utils.data import DataLoader
from tqdm import trange
from tqdm import tqdm


def classifier_training(train, test, new):


    transforms = tf.Compose([
        tf.Resize(32),
        tf.ToTensor()
    ])
    batch_size = 8
    mnist_train = torchvision.datasets.MNIST(root='data/',
                                             train=True,
                                             transform=transforms,
                                             download=True)

    mnist_test = torchvision.datasets.MNIST(root='data/',
                                            train=False,
                                            transform=transforms,
                                            download=True)

    train_loader = DataLoader(dataset=new,
                              batch_size=8,
                              shuffle=True,
                              drop_last=True)

    test_loader = DataLoader(dataset=test,
                             batch_size=8,
                             shuffle=False,
                             drop_last=False)

    model = CNNClassifier()
    model = model.cuda(0)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)
    images = new.images
    label = new.labels
    n = images.size(0)
    best_accuracy = 0.
    for epoch in trange(200):

        model.train()
        x, y = None, None
        # pbar = tqdm(enumerate(train_loader), total=len(train_loader))
        # for step, (x, y) in pbar:
        max = (n // batch_size)
        if n % batch_size != 0:
            max += 1
        model.eval()
        for i in range(max):
            if i == max - 1:
                x, y = images[batch_size * i:n, :, :, :].clone().detach(), label[batch_size * i:n].clone().detach()
            else:
                x, y = images[batch_size * i:batch_size * (i+1), :, :, :].clone().detach(), label[batch_size * i:batch_size * (i+1)].clone().detach()
            x.cuda(0), y.cuda(0)

            optimizer.zero_grad()
            model = model.to(device)
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()

        correct = 0
        model.eval()
        for (x, y) in test_loader:
            x, y = x.cuda(0), y.cuda(0)

            with torch.no_grad():
                logits = model(x)
                correct += (torch.argmax(logits, 1) == y).sum()

        accuracy = correct / len(mnist_test)
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), f'checkpoints_test/best.pt')
            print(f'[Epoch : {epoch}/200] Best Accuracy : {accuracy:.6f}%')


def training(train_dataset, test_dataset):
    model = CNNClassifier()
    model = model.cuda(0)

    test_loader = DataLoader(dataset=test_dataset,
                             batch_size=8,
                             shuffle=False,
                             drop_last=False)

    train_loader = DataLoader(dataset=train_dataset,
                              batch_size=8,
                              shuffle=False,
                              drop_last=True
                              )

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)

    best_accuracy = 0.
    for epoch in trange(75):

        model.train()
        x, y = None, None
        pbar = tqdm(enumerate(train_loader), total=len(train_loader))
        for step, (x, y) in pbar:
            x, y = x.cuda(0), y.cuda(0)

            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y.to(torch.int64))
            loss.backward()
            optimizer.step()

        correct = 0
        model.eval()
        for (x, y) in test_loader:
            x, y = x.cuda(0), y.cuda(0)

            with torch.no_grad():
                logits = model(x)
                correct += (torch.argmax(logits, 1) == y).sum()

        accuracy = correct / len(train_dataset)
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), f'new_checkpoints/best_1.0.pt')
            print(f'[Epoch : {epoch}/200] Best Accuracy : {accuracy:.6f}%')


def denormalize(images):
    out = (images + 1) / 2
    return out.clamp(0, 1)


"""# Make Dataset"""


def make_dataset(images, labels, conf, fake_images=None, fake_labels=None, fake_conf=None, real_ratio=1.0):
    # Images: (60000, 28, 28)
    # labels: (60000, 1)
    # conf:   (60000, 1)
    device = torch.device('cpu')
    if torch.cuda.is_available():
        device = torch.cuda.current_device()

    N = labels.size(0)
    fake_N = fake_labels.size(0)

    # """Shuffle the fake dataset"""
    indices = torch.randperm(fake_N)
    fake_labels = fake_labels[indices]
    fake_conf = fake_conf[indices]
    fake_images = fake_images[indices]

    """Combine the 2 Dataset"""
    # idx = np.random.choice(N, real_ratio * N, replace=False)
    # fake_idx = np.random.choice(N, (1 - real_ratio) * N, replace=False)
    idx = int(real_ratio * N)
    fake_idx = int((1 - real_ratio) * idx)
    image_data = images[:idx, :, :].to(device)
    image_data = torch.unsqueeze(image_data, dim=1)
    label_data = labels[:idx].to(device)
    conf = conf[:idx].to(device)
    _, counts = torch.unique(label_data, sorted=True, return_counts=True)
    print("counts", counts)
    save_image(denormalize(image_data[0:5, :, :, :]),
               os.path.join("figure", "real_image_train_.png"))

    fake_imageData = fake_images[:fake_idx, :, :].to(device)
    fake_imageData = torch.unsqueeze(fake_imageData, dim=1)
    fake_labelData = fake_labels[:fake_idx].to(device)
    fake_conf = fake_conf[:fake_idx].to(device)
    _, fake_counts = torch.unique(fake_labelData, sorted=True, return_counts=True)
    print("counts", fake_counts)
    # save_image(denormalize(fake_imageData[0:5, :, :, :]),
    #            os.path.join("figure", "fake_image_train_.png"))

    new_images = torch.cat((image_data, fake_imageData), dim=0)
    new_labels = torch.cat((label_data, fake_labelData), dim=0)
    new_confs = torch.cat((conf, fake_conf), dim=0)

    """Sort by confidence"""
    n = torch.argsort(-new_confs).to(device)
    new_images = new_images[n, :, :, :]
    new_labels = new_labels[n]
    # new_images = torch.unsqueeze(new_images, dim=1)
    new_labels = new_labels.to(torch.int64)
    dataset = NewSimpleDataset(new_images, new_labels)
    save_image(denormalize(new_images[N - 50:N, :, :, :]),
               os.path.join("figure", "new_image_train_.png"))
    print(new_labels[N - 50:N])
    return dataset


"""# Generate Images """


def generate_images(generator, config, n=60000):
    device = torch.device('cpu')
    if torch.cuda.is_available():
        device = torch.cuda.current_device()
        # generator = torch.nn.DataParallel(generator.to(device))
    generator = generator.to(device)

    ones = torch.ones(n // 10)
    ones = ones.long()
    ones = ones.to(device)
    fake_images = torch.Tensor()
    for i in range(10):
        z = torch.randn(n // 10, config.latent_size).to(device)
        # fake_y = torch.tensor(, device=device).long()
        with torch.no_grad():
            fake_images = fake_images.to(device)
            fake_images = torch.cat([fake_images, generator(z, ones * i)], dim=0)
    fake_images = torch.squeeze(fake_images, dim=1)
    fake_y = torch.cat([ones * 0, ones * 1, ones * 2, ones * 3, ones * 4,
                        ones * 5, ones * 6, ones * 7, ones * 8, ones * 9], dim=0).long().to(device)

    return fake_images, fake_y


"""# Get Confidence"""


def classify(model, input_images):
    device = torch.device('cpu')
    if torch.cuda.is_available():
        device = torch.cuda.current_device()

    model = model.to(device)
    images = input_images.clone().detach()
    images = images.type(torch.FloatTensor)
    images = images.to(device)
    images = images.unsqueeze(dim=1)
    n = images.size(0)
    batch_size = 8
    num_class = 10
    conf = torch.zeros(n, num_class)
    labels = torch.zeros(n)
    max = (n // batch_size)
    if n % batch_size != 0:
        max += 1

    soft_max = nn.Softmax(dim=1).to(device)

    model.eval()
    with torch.no_grad():
        for i in range(max):
            if i == max - 1:
                temp_conf = soft_max(model(images[batch_size * i:n, :, :, :]))
                conf[batch_size * i:n, :] = temp_conf
                labels[batch_size * i:n] = torch.argmax(temp_conf, 1)
                break
            else:
                temp_conf = soft_max(model(images[batch_size * i:batch_size * (i + 1), :, :, :]))
                conf[batch_size * i:batch_size * (i + 1), :] = temp_conf
                labels[batch_size * i:batch_size * (i + 1)] = torch.argmax(temp_conf, 1)

    return conf.to(device), labels.to(device)


"""# Main"""

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)

if __name__ == '__main__':
    set_seed(42)
    device = torch.device('cpu')
    if torch.cuda.is_available():
        device = torch.cuda.current_device()

    tyaml = yaml.load(open(f'config/train.yaml', 'r'), Loader=yaml.FullLoader)
    tconf = TrainConfig(tyaml)

    images = np.load(f'data/{tyaml["dataset"]}_images.npy')
    labels = np.load(f'data/{tyaml["dataset"]}_labels.npy')

    transf = tf.Compose([
        tf.Resize(32),
        tf.ToTensor()
    ])
    mnist_train = torchvision.datasets.MNIST(root='data/',
                                             train=True,
                                             transform=transf,
                                             download=True)

    mnist_test = torchvision.datasets.MNIST(root='data/',
                                            train=False,
                                            transform=transf,
                                            download=True)
    print((mnist_train.data[0] > 0).nonzero())
    train_loader = DataLoader(dataset=mnist_train,
                              batch_size=8,
                              shuffle=False,
                              drop_last=True)

    test_loader = DataLoader(dataset=mnist_test,
                             batch_size=8,
                             shuffle=False,
                             drop_last=False)

    """Generate images"""
    myaml = yaml.load(open('config/model.yaml', 'r'), Loader=yaml.FullLoader)
    mconf = ModelConfig(myaml)
    linear_generator = LinearGenerator(mconf)
    conv_generator = ConvolutionGenerator(mconf)

    generator = conv_generator
    generator.load_state_dict(torch.load(
        'weights/200/G_200.ckpt'))
    # generator.load_state_dict(torch.load(
    #     'weights/200/G_200.ckpt', map_location=torch.device('cpu')))
    num_fake_images = 60000
    fake_images, fake_y = generate_images(generator, tconf, num_fake_images)
    # fake_images, fake_y = torch.zeros((2000, 32, 32)), torch.zeros((2000))
    fake_images = denormalize(fake_images)

    classifier = CNNClassifier()
    classifier.load_state_dict(torch.load(
        'classifier/checkpoints/best.pt'))
    # classifier.load_state_dict(torch.load(
    #     'classifier/checkpoints/best.pt', map_location=torch.device('cpu')))
    images_for_dataset = tf.Resize(32)(mnist_train.data)

    images_for_dataset = torch.unsqueeze(images_for_dataset, dim=1)
    newset = NewSimpleDataset(images_for_dataset, mnist_train.targets)
    classifier_training(mnist_train, mnist_test, newset)

    """Get Confidence value"""
    fake_conf, fake_labels = classify(classifier, fake_images)
    wrong_indices = (fake_labels != fake_y).nonzero()
    wrong_conf = torch.max(fake_conf[wrong_indices], dim=1)
    right_indices = (fake_labels == fake_y).nonzero()
    right_indices = torch.squeeze(right_indices, dim=1)
    fake_conf = fake_conf[right_indices, :]
    fake_images = fake_images[right_indices, :, :]
    fake_labels = fake_labels[right_indices]
    print(f"{wrong_indices.size(0)} wrong labels: ", wrong_indices)

    conf, predicted_labels = classify(classifier, images_for_dataset)
    fake_conf = fake_conf.to(torch.float)
    conf = conf.to(torch.float)
    """Mix the Dataset"""
    real_ratio = 1.0  # [0.1, 0.2, 0.5, 1.0]
    # hello = torch.max(fake_conf, dim=1).values
    # hello2 = torch.max(conf, dim=1).values
    # hard_indices = (1.0 > hello).nonzero()
    # hard_index = (0.0 < hello[hard_indices]).nonzero()
    # zero_indices = (hello == 0).nonzero()
    # print("hard examples labels: ", hard_index)
    # print(hello[hard_index])
    # print(hello[zero_indices])
    # print(torch.max(hello[hard_index]), torch.min(hello[hard_index]), torch.mean(hello[hard_index], dim=0))

    dataset = make_dataset(images_for_dataset, mnist_train.targets, torch.max(conf, dim=1).values, fake_images,
                           fake_labels, torch.max(fake_conf, dim=1).values, real_ratio)

    new_train_loader = DataLoader(dataset=dataset,
                                  batch_size=8,
                                  shuffle=False,
                                  drop_last=True
                                  )
    """Training Classifier """
    # training(mnist_train, mnist_test)
    classifier_training(mnist_train, mnist_test, dataset)

    """Evaluation"""
    model = CNNClassifier()
    model.load_state_dict(torch.load(
        'new_checkpoints/best.pt'))
    correct = 0
    model.to(device)
    model.eval()
    for (x, y) in test_loader:
        x, y = x.cuda(0), y.cuda(0)

        with torch.no_grad():
            logits = model(x)
            correct += (torch.argmax(logits, 1) == y).sum()
    accuracy = correct / len(dataset)
    print(f' Best Accuracy : {accuracy:.6f}%')

    correct = 0
    for (x, y) in new_train_loader:
        x, y = x.cuda(0), y.cuda(0)

        with torch.no_grad():
            logits = model(x)
            correct += (torch.argmax(logits, 1) == y).sum()

    accuracy = correct / len(dataset)
    print(f' Best Accuracy : {accuracy:.6f}%')
